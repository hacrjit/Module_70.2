{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "572f756e-dc5b-4644-8021-61f787c14b84",
   "metadata": {},
   "source": [
    "### <b>Question No. 1</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644e5cb0-6bf8-4067-9b0e-d4b341a85b16",
   "metadata": {},
   "source": [
    "In machine learning algorithms, kernel functions are used to implicitly map input data into a higher-dimensional feature space, where the data points are more easily separable. Polynomial functions are a type of kernel function commonly used for this purpose.\n",
    "\n",
    "The relationship between polynomial functions and kernel functions lies in the way polynomial kernel functions transform the input data. A polynomial kernel function of degree \\(d\\) computes the dot product of the transformed feature vectors in the higher-dimensional space. This computation is equivalent to a polynomial function of degree \\(d\\) applied to the original input features.\n",
    "\n",
    "Mathematically, the polynomial kernel function can be defined as:\n",
    "\n",
    "K(x, x') = (x â‹… x' + c)^d\n",
    "\n",
    "where:\n",
    "- x and x' are the input feature vectors,\n",
    "- c is a constant term,\n",
    "- d is the degree of the polynomial.\n",
    "\n",
    "By using polynomial kernel functions, machine learning algorithms such as Support Vector Machines (SVM) can effectively learn nonlinear decision boundaries in the original input space without explicitly computing the transformations to the higher-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b0ac7d-9080-45dc-b170-dd1e8c2e18a2",
   "metadata": {},
   "source": [
    "### <b>Question No. 2</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0369be-5e7d-4648-9012-9f27bd015244",
   "metadata": {},
   "source": [
    "To implement an SVM with a polynomial kernel in Python using Scikit-learn, you can use the `SVC` class from the `sklearn.svm` module. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e3228de-8698-4b35-a479-f993e0c116d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create an SVM classifier with a polynomial kernel\n",
    "# You can specify the degree of the polynomial using the 'degree' parameter\n",
    "clf = SVC(kernel='poly', degree=3)  # Using a 3rd degree polynomial kernel\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72043dc9-1834-4474-ad33-bd78722a851a",
   "metadata": {},
   "source": [
    "In this example, we use the `SVC` class with `kernel='poly'` to specify that we want to use a polynomial kernel. The `degree` parameter is set to 3, indicating that we want to use a 3rd degree polynomial kernel. You can adjust the `degree` parameter to use a different degree of the polynomial kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf2561-5cd2-4782-8f3a-3ada37a31e9b",
   "metadata": {},
   "source": [
    "### <b>Question No. 3</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da44ed85-0287-4579-be4b-0b44ee59b4b5",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), epsilon (\\( \\epsilon \\)) is a hyperparameter that controls the width of the margin around the predicted value within which no penalty is associated with errors. It defines a tube within which the regression line must lie.\n",
    "\n",
    "Increasing the value of epsilon in SVR can affect the number of support vectors in the following ways:\n",
    "\n",
    "1. **More Support Vectors:** Increasing epsilon allows for a larger margin of error, which means that more data points can be within the margin without penalty. This can lead to a larger number of support vectors, as the model tries to fit the data within the wider margin.\n",
    "\n",
    "2. **Fewer Support Vectors:** Conversely, in some cases, increasing epsilon can lead to fewer support vectors. This can happen when the wider margin allows the model to generalize better to the data, reducing the need for many support vectors to fit the data points exactly.\n",
    "\n",
    "The impact of epsilon on the number of support vectors can vary depending on the dataset and the complexity of the underlying relationship between the features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2cb79-9671-4ce6-aa0b-46a95ee42458",
   "metadata": {},
   "source": [
    "### <b>Question No. 4</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f31a43-dd00-46e6-8010-e5924eb7a714",
   "metadata": {},
   "source": [
    "The performance of Support Vector Regression (SVR) is influenced by several key parameters, including the choice of kernel function, C parameter, epsilon parameter, and gamma parameter. Here's a brief explanation of each parameter and how it affects SVR:\n",
    "\n",
    "1. **Kernel Function:**\n",
    "   - The kernel function determines the type of transformation applied to the input features to map them into a higher-dimensional space.\n",
    "   - Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "   - Choice of kernel function affects the model's ability to capture complex relationships in the data. For example, RBF kernel is suitable for non-linear relationships.\n",
    "\n",
    "2. **C Parameter:**\n",
    "   - The C parameter controls the trade-off between achieving a low error on the training data and maximizing the margin.\n",
    "   - A smaller C value allows for a wider margin and more tolerance for errors (soft margin), which can prevent overfitting.\n",
    "   - A larger C value penalizes errors more heavily, leading to a narrower margin and potentially better performance on the training data but with a higher risk of overfitting.\n",
    "\n",
    "3. **Epsilon Parameter:**\n",
    "   - The epsilon parameter (epsilon) defines the margin of tolerance where no penalty is given to errors.\n",
    "   - It determines the width of the tube within which predictions are considered acceptable.\n",
    "   - Increasing epsilon allows for a wider tube and more tolerance for errors, which can lead to smoother and more generalized predictions.\n",
    "\n",
    "4. **Gamma Parameter:**\n",
    "   - The gamma parameter (gamma) defines the influence of a single training example, with low values meaning 'far' and high values meaning 'close'.\n",
    "   - A low gamma value means that points far away from the decision boundary have a high influence, leading to a smoother decision boundary.\n",
    "   - A high gamma value means that only points close to the decision boundary have a high influence, which can result in a more complex decision boundary and potentially overfitting.\n",
    "\n",
    "Example scenarios for adjusting these parameters:\n",
    "- Increase C if you want to reduce the margin of tolerance for errors and prioritize correctly fitting the training data.\n",
    "- Increase epsilon if you want to allow for more tolerance for errors and prioritize a smoother, more generalized model.\n",
    "- Increase gamma if you want to create a more complex decision boundary that closely fits the training data, but be cautious of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1340d6a1-8935-456b-afda-416cf38776a8",
   "metadata": {},
   "source": [
    "### <b>Question No. 5</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e1eb7fb-51bf-439b-87b5-c3bd73ad521d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['best_svc_model.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Loading the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Preprocessing the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Creating an instance of the SVC classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Training the classifier on the training data\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Using the trained classifier to predict the labels of the testing data\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "# Evaluating the performance of the classifier using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Tuning the hyperparameters of the SVC classifier using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 0.01, 0.001], 'kernel': ['linear', 'rbf', 'poly']}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Training the tuned classifier on the entire dataset\n",
    "best_svc = grid_search.best_estimator_\n",
    "best_svc.fit(scaler.transform(X), y)\n",
    "\n",
    "# Saving the trained classifier to a file for future use\n",
    "joblib.dump(best_svc, 'best_svc_model.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
